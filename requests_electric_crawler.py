import requests
from bs4 import BeautifulSoup
import json
import time
from typing import Dict, List


class RequestsEVCrawler:
    def __init__(self):
        self.base_url = "https://ev.or.kr"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def get_session_cookies(self):
        """ì„¸ì…˜ ì¿ í‚¤ ì–»ê¸°"""
        try:
            # 1. ë©”ì¸ í˜ì´ì§€ ì ‘ì†
            print("ğŸ“ ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì¤‘...")
            main_response = self.session.get(f"{self.base_url}/nportal/main.do")
            print(f"   ìƒíƒœ ì½”ë“œ: {main_response.status_code}")
            
            # 2. êµ¬ë§¤ë³´ì¡°ê¸ˆ ì§€ê¸‰í˜„í™© í˜ì´ì§€ ì ‘ì†
            print("ğŸ“ êµ¬ë§¤ë³´ì¡°ê¸ˆ ì§€ê¸‰í˜„í™© í˜ì´ì§€ ì ‘ì† ì¤‘...")
            subsidy_url = f"{self.base_url}/nportal/buySupprt/initSubsidyPaymentCheckAction.do"
            subsidy_response = self.session.get(subsidy_url)
            print(f"   ìƒíƒœ ì½”ë“œ: {subsidy_response.status_code}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def get_local_car_price_list(self, year="2025", car_type="11"):
        """ì§€ìì²´ ì°¨ì¢…ë³„ ë³´ì¡°ê¸ˆ ëª©ë¡ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # psPopupLocalCarPirce.do í˜ì´ì§€ë¥¼ POSTë¡œ ì ‘ì†
            url = f"{self.base_url}/nportal/buySupprt/psPopupLocalCarPirce.do"
            
            # í¼ ë°ì´í„° ì„¤ì •
            data = {
                'year1': year,
                'car_type': car_type  # 11: ìŠ¹ìš©
            }
            
            headers = {
                'Content-Type': 'application/x-www-form-urlencoded',
                'Referer': f"{self.base_url}/nportal/buySupprt/initSubsidyPaymentCheckAction.do",
                'Origin': self.base_url
            }
            
            print(f"ğŸ“ ì§€ìì²´ ë³´ì¡°ê¸ˆ ëª©ë¡ í˜ì´ì§€ ì ‘ì† ì¤‘...")
            response = self.session.post(url, data=data, headers=headers)
            print(f"   ìƒíƒœ ì½”ë“œ: {response.status_code}")
            
            if response.status_code == 200:
                # ë””ë²„ê¹…ì„ ìœ„í•´ HTML ì €ì¥
                with open('debug_list_page.html', 'w', encoding='utf-8') as f:
                    f.write(response.text)
                return response.text
            
            return None
            
        except Exception as e:
            print(f"âŒ ëª©ë¡ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {e}")
            return None
    
    def extract_regions_from_list(self, html_content):
        """ëª©ë¡ í˜ì´ì§€ì—ì„œ ì§€ì—­ ì •ë³´ ì¶”ì¶œ"""
        soup = BeautifulSoup(html_content, 'html.parser')
        regions = []
        
        # tbodyì—ì„œ ì§€ì—­ ì •ë³´ ì°¾ê¸°
        tbody = soup.find('tbody')
        if tbody:
            rows = tbody.find_all('tr')
            for row in rows:
                cells = row.find_all('td')
                if len(cells) >= 3:
                    # ì²« ë²ˆì§¸ ì…€: ì§€ì—­ëª…
                    local_name = cells[0].get_text(strip=True)
                    
                    # onclick ì†ì„±ì—ì„œ ì§€ì—­ ì½”ë“œ ì¶”ì¶œ
                    onclick_elem = row.find('a', onclick=True)
                    if onclick_elem:
                        onclick_str = onclick_elem.get('onclick', '')
                        # goLocalCarPirce('1100','11','ì„œìš¸íŠ¹ë³„ì‹œ') í˜•íƒœì—ì„œ ì¶”ì¶œ
                        import re
                        match = re.search(r"goLocalCarPirce\('(\d+)'", onclick_str)
                        if match:
                            local_code = match.group(1)
                            regions.append({
                                'code': local_code,
                                'name': local_name
                            })
                            print(f"   ë°œê²¬: {local_name} ({local_code})")
        
        return regions
    
    def get_local_car_detail(self, year="2025", local_cd="1100", car_type="11", local_nm="ì„œìš¸íŠ¹ë³„ì‹œ"):
        """íŠ¹ì • ì§€ì—­ì˜ ì°¨ëŸ‰ë³„ ë³´ì¡°ê¸ˆ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # psPopupLocalCarModelPrice.doë¡œ POST ìš”ì²­
            url = f"{self.base_url}/nportal/buySupprt/psPopupLocalCarModelPrice.do"
            
            data = {
                'year': year,
                'local_cd': local_cd,
                'car_type': car_type,
                'local_nm': local_nm
            }
            
            headers = {
                'Content-Type': 'application/x-www-form-urlencoded',
                'Referer': f"{self.base_url}/nportal/buySupprt/psPopupLocalCarPirce.do",
                'Origin': self.base_url,
                'X-Requested-With': 'XMLHttpRequest'
            }
            
            response = self.session.post(url, data=data, headers=headers)
            
            if response.status_code == 200:
                return response.text
            else:
                print(f"   âŒ ìƒì„¸ í˜ì´ì§€ ì‘ë‹µ ì‹¤íŒ¨: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"   âŒ ìƒì„¸ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None
    
    def parse_vehicle_data(self, html_content):
        """HTMLì—ì„œ ì°¨ëŸ‰ ë°ì´í„° íŒŒì‹±"""
        soup = BeautifulSoup(html_content, 'html.parser')
        vehicles = []
        
        # í…Œì´ë¸” ì°¾ê¸°
        table = soup.find('table', class_='table01')
        if not table:
            table = soup.find('table')
        
        if table:
            # í—¤ë” ë¶„ì„
            headers = []
            thead = table.find('thead')
            if thead:
                header_row = thead.find('tr')
                if header_row:
                    for th in header_row.find_all(['th', 'td']):
                        headers.append(th.get_text(strip=True))
            
            # ë°ì´í„° í–‰ ì²˜ë¦¬
            tbody = table.find('tbody')
            if tbody:
                rows = tbody.find_all('tr')
                for row in rows:
                    cells = row.find_all('td')
                    if len(cells) >= 3:
                        row_data = [cell.get_text(strip=True) for cell in cells]
                        
                        # ë¹ˆ ë°ì´í„°ë‚˜ "ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤" ì œì™¸
                        if row_data[0] and 'ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤' not in ''.join(row_data):
                            vehicle = self.map_vehicle_data(headers, row_data)
                            if vehicle.get('manufacturer') and vehicle.get('model'):
                                vehicles.append(vehicle)
        
        return vehicles
    
    def map_vehicle_data(self, headers, row_data):
        """í—¤ë”ì™€ í–‰ ë°ì´í„°ë¥¼ ë§¤í•‘í•˜ì—¬ ì°¨ëŸ‰ ì •ë³´ ìƒì„±"""
        vehicle = {}
        
        # ì¸ë±ìŠ¤ ì°¾ê¸°
        manufacturer_idx = 0
        model_idx = 1
        detail_idx = 2
        
        for i, h in enumerate(headers):
            if 'ì œì¡°ì‚¬' in h:
                manufacturer_idx = i
            elif 'ì°¨ì¢…' in h and 'ëª¨ë¸ëª…' not in h:
                model_idx = i
            elif 'ëª¨ë¸ëª…' in h:
                detail_idx = i
        
        # ë°ì´í„° ë§¤í•‘
        if manufacturer_idx < len(row_data):
            vehicle['manufacturer'] = row_data[manufacturer_idx]
        if model_idx < len(row_data):
            vehicle['model'] = row_data[model_idx]
        if detail_idx < len(row_data) and detail_idx != model_idx:
            vehicle['model_detail'] = row_data[detail_idx]
        
        # ë³´ì¡°ê¸ˆ ì •ë³´
        for i, h in enumerate(headers):
            if i < len(row_data):
                if 'êµ­ë¹„' in h and 'ë§Œì›' in h:
                    vehicle['national_subsidy'] = row_data[i]
                elif 'ì§€ë°©ë¹„' in h and 'ë§Œì›' in h:
                    vehicle['local_subsidy'] = row_data[i]
                elif ('ë³´ì¡°ê¸ˆ' in h or 'í•©ê³„' in h) and 'ë§Œì›' in h and 'êµ­ë¹„' not in h and 'ì§€ë°©ë¹„' not in h:
                    vehicle['total_subsidy'] = row_data[i]
        
        return vehicle
    
    def crawl_all_regions(self, year="2025", car_type="11"):
        """ëª¨ë“  ì§€ì—­ì˜ ë³´ì¡°ê¸ˆ ë°ì´í„° í¬ë¡¤ë§"""
        print("ğŸš€ Requests ê¸°ë°˜ ì „ê¸°ì°¨ ë³´ì¡°ê¸ˆ í¬ë¡¤ë§ ì‹œì‘...")
        
        # 1. ì„¸ì…˜ ì´ˆê¸°í™”
        if not self.get_session_cookies():
            return None
        
        # 2. ì§€ì—­ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        list_html = self.get_local_car_price_list(year, car_type)
        if not list_html:
            print("âŒ ì§€ì—­ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        # 3. ì§€ì—­ ì •ë³´ ì¶”ì¶œ
        regions = self.extract_regions_from_list(list_html)
        if not regions:
            print("âš ï¸ ì§€ì—­ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì§€ì—­ ì‚¬ìš©")
            regions = [
                {'code': '1100', 'name': 'ì„œìš¸íŠ¹ë³„ì‹œ'},
                {'code': '2600', 'name': 'ë¶€ì‚°ê´‘ì—­ì‹œ'},
                {'code': '2700', 'name': 'ëŒ€êµ¬ê´‘ì—­ì‹œ'},
                {'code': '2800', 'name': 'ì¸ì²œê´‘ì—­ì‹œ'},
                {'code': '2900', 'name': 'ê´‘ì£¼ê´‘ì—­ì‹œ'}
            ]
        
        print(f"\nğŸ“ ì´ {len(regions)}ê°œ ì§€ì—­ ë°œê²¬")
        
        all_data = {}
        
        # 4. ê° ì§€ì—­ë³„ ë°ì´í„° ìˆ˜ì§‘
        for i, region in enumerate(regions[:10]):  # ì²˜ìŒ 10ê°œ ì§€ì—­ë§Œ
            local_cd = region['code']
            local_nm = region['name']
            
            print(f"\nğŸ” [{i+1}/{min(len(regions), 10)}] {local_nm} ({local_cd}) ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            
            # ìƒì„¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            detail_html = self.get_local_car_detail(year, local_cd, car_type, local_nm)
            
            if detail_html:
                # ë°ì´í„° íŒŒì‹±
                vehicles = self.parse_vehicle_data(detail_html)
                
                if vehicles:
                    all_data[local_nm] = vehicles
                    print(f"   âœ… {len(vehicles)}ê°œ ì°¨ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
                    # ìƒ˜í”Œ ì¶œë ¥
                    for v in vehicles[:2]:
                        print(f"      - {v.get('manufacturer')} {v.get('model_detail', v.get('model'))}: {v.get('total_subsidy', 'N/A')}ë§Œì›")
                else:
                    print(f"   âš ï¸ ë°ì´í„° ì—†ìŒ")
            else:
                print(f"   âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨")
            
            # ìš”ì²­ ê°„ê²©
            time.sleep(1)
        
        return all_data
    
    def save_results(self, data, filename="ev_subsidy_requests.json"):
        """ê²°ê³¼ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename}")
    
    def run(self):
        """ì‹¤í–‰"""
        data = self.crawl_all_regions("2025", "11")
        
        if data:
            total_count = sum(len(vehicles) for vehicles in data.values())
            print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ì´ {len(data)}ê°œ ì§€ì—­, {total_count}ê°œ ì°¨ëŸ‰ ë°ì´í„° ìˆ˜ì§‘")
            
            # ê²°ê³¼ ì €ì¥
            self.save_results(data)
            
            return data
        else:
            print("\nâŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
            return None


if __name__ == "__main__":
    crawler = RequestsEVCrawler()
    crawler.run()